<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>26ccb5cdc5e14103aef83da2962e934d</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div id="present-brown" class="cell markdown" id="present-brown">
<h1 id="hw06-transformers-and-doc-embeddings">HW06: Transformers and Doc
Embeddings</h1>
<p>Remember that these homework work as a completion grade. <strong>You
can skip one section of this homework.</strong></p>
</div>
<div id="irish-ending" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:403}"
id="irish-ending" data-outputId="5b9aeb9b-90f0-4642-82b1-bc90c5cef19b">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget https:<span class="op">//</span>raw.githubusercontent.com<span class="op">/</span>mhjabreel<span class="op">/</span>CharCnn_Keras<span class="op">/</span>master<span class="op">/</span>data<span class="op">/</span>ag_news_csv<span class="op">/</span>train.csv</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;train.csv&#39;</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df.columns <span class="op">=</span> [<span class="st">&quot;label&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;lead&quot;</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>label_map <span class="op">=</span> {<span class="dv">1</span>:<span class="st">&quot;world&quot;</span>, <span class="dv">2</span>:<span class="st">&quot;sport&quot;</span>, <span class="dv">3</span>:<span class="st">&quot;business&quot;</span>, <span class="dv">4</span>:<span class="st">&quot;sci/tech&quot;</span>}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_label(x):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>	<span class="cf">return</span> label_map[x]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;label&quot;</span>] <span class="op">=</span> df[<span class="st">&quot;label&quot;</span>].<span class="bu">apply</span>(replace_label) </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&quot;text&quot;</span>] <span class="op">=</span> df[<span class="st">&quot;title&quot;</span>] <span class="op">+</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> df[<span class="st">&quot;lead&quot;</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.sample(n<span class="op">=</span><span class="dv">10000</span>) <span class="co"># # only use 10K datapoints</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>--2023-04-05 07:51:11--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 29470338 (28M) [text/plain]
Saving to: ‘train.csv’

train.csv           100%[===================&gt;]  28.10M   161MB/s    in 0.2s    

2023-04-05 07:51:11 (161 MB/s) - ‘train.csv’ saved [29470338/29470338]

</code></pre>
</div>
<div class="output execute_result" data-execution_count="1">

  <div id="df-bd786aed-a0c6-4be4-b490-f8ed27523f64">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>title</th>
      <th>lead</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>90289</th>
      <td>sport</td>
      <td>Holyfield can #39;t see the end</td>
      <td>Evander Holyfield may have finally run out of ...</td>
      <td>Holyfield can #39;t see the end Evander Holyfi...</td>
    </tr>
    <tr>
      <th>57351</th>
      <td>sci/tech</td>
      <td>IBM Fashions New Blades (NewsFactor)</td>
      <td>NewsFactor - IBM (NYSE: IBM) is upgrading its ...</td>
      <td>IBM Fashions New Blades (NewsFactor) NewsFacto...</td>
    </tr>
    <tr>
      <th>74186</th>
      <td>world</td>
      <td>Lawmakers Back Sharon on Plan for Leaving Gaza</td>
      <td>The vote, in an atmosphere of high drama and t...</td>
      <td>Lawmakers Back Sharon on Plan for Leaving Gaza...</td>
    </tr>
    <tr>
      <th>55112</th>
      <td>sport</td>
      <td>Works in Progress</td>
      <td>U-Md.'s Joel Statham and Georgia Tech's Reggie...</td>
      <td>Works in Progress U-Md.'s Joel Statham and Geo...</td>
    </tr>
    <tr>
      <th>93914</th>
      <td>business</td>
      <td>US Economy: Consumer Prices, Industrial Produc...</td>
      <td>US consumer prices rose in October, spurred by...</td>
      <td>US Economy: Consumer Prices, Industrial Produc...</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-bd786aed-a0c6-4be4-b490-f8ed27523f64')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-bd786aed-a0c6-4be4-b490-f8ed27523f64 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-bd786aed-a0c6-4be4-b490-f8ed27523f64');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div id="regulated-klein" class="cell markdown" id="regulated-klein">
<h2 id="hugginface-transformers">Hugginface Transformers</h2>
</div>
<div id="reasonable-graph" class="cell code" id="reasonable-graph">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install transformers</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code></pre></div>
</div>
<div id="ACF4bjkham9e" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ACF4bjkham9e" data-outputId="885847fd-e3d5-4b4c-d316-2115061d4869">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (device)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>cuda
</code></pre>
</div>
</div>
<div id="confident-village" class="cell code" id="confident-village">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO build a transformer model to do sequence classification with the goal to predict the label from the text</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">&#39;distilbert-base-uncased&#39;</span> <span class="co"># huggingface model_ID or path to folder </span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(<span class="st">&#39;distilbert-base-uncased&#39;</span>, num_labels<span class="op">=</span><span class="dv">4</span>)</span></code></pre></div>
</div>
<div id="psychological-object" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="psychological-object"
data-outputId="512056ce-7c74-45b8-b3e2-eea0861e1eb6">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO print the summary of the model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>DistilBertForSequenceClassification(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0-5): 6 x TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
</code></pre>
</div>
</div>
<div id="f35b0eMRf49M" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:206}"
id="f35b0eMRf49M" data-outputId="6f3d5a6a-5bd0-46ad-ab84-3f1b48c2a0a3">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO prepare the dataset for torch.</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">## transform labels</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>cat_to_int <span class="op">=</span> {cat: i <span class="cf">for</span> i, cat <span class="kw">in</span> <span class="bu">enumerate</span>(df[<span class="st">&#39;label&#39;</span>].unique())}</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;label_int&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;label&#39;</span>].<span class="bu">map</span>(cat_to_int)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="23">

  <div id="df-a6c8af53-a493-49a0-ad4d-9d58c29c78a5">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>title</th>
      <th>lead</th>
      <th>text</th>
      <th>label_int</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>90289</th>
      <td>sport</td>
      <td>Holyfield can #39;t see the end</td>
      <td>Evander Holyfield may have finally run out of ...</td>
      <td>Holyfield can #39;t see the end Evander Holyfi...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>57351</th>
      <td>sci/tech</td>
      <td>IBM Fashions New Blades (NewsFactor)</td>
      <td>NewsFactor - IBM (NYSE: IBM) is upgrading its ...</td>
      <td>IBM Fashions New Blades (NewsFactor) NewsFacto...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>74186</th>
      <td>world</td>
      <td>Lawmakers Back Sharon on Plan for Leaving Gaza</td>
      <td>The vote, in an atmosphere of high drama and t...</td>
      <td>Lawmakers Back Sharon on Plan for Leaving Gaza...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>55112</th>
      <td>sport</td>
      <td>Works in Progress</td>
      <td>U-Md.'s Joel Statham and Georgia Tech's Reggie...</td>
      <td>Works in Progress U-Md.'s Joel Statham and Geo...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>93914</th>
      <td>business</td>
      <td>US Economy: Consumer Prices, Industrial Produc...</td>
      <td>US consumer prices rose in October, spurred by...</td>
      <td>US Economy: Consumer Prices, Industrial Produc...</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-a6c8af53-a493-49a0-ad4d-9d58c29c78a5')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-a6c8af53-a493-49a0-ad4d-9d58c29c78a5 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-a6c8af53-a493-49a0-ad4d-9d58c29c78a5');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div id="statistical-recommendation" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="statistical-recommendation"
data-outputId="5df8ad76-8036-42e2-8592-a1462f6bddd9">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO prepare the dataset for torch.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> DistilBertTokenizerFast.from_pretrained(model_name)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(df[<span class="st">&#39;text&#39;</span>].tolist(), return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> torch.tensor(df[<span class="st">&#39;label_int&#39;</span>].tolist()).<span class="bu">long</span>() </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#print(inputs, labels)</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>tensor([0, 1, 2,  ..., 1, 3, 2])
</code></pre>
</div>
</div>
<div id="2StSU7img-sc" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="2StSU7img-sc" data-outputId="6a8d576a-6ff5-4c4a-883a-ee0382580ef1">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>unique_labels, counts <span class="op">=</span> np.unique(df[<span class="st">&quot;label_int&quot;</span>], return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (unique_labels, counts)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(<span class="st">&#39;distilbert-base-uncased&#39;</span>, num_labels<span class="op">=</span><span class="bu">len</span>(unique_labels))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam([</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&#39;params&#39;</span>: model.distilbert.parameters(), <span class="st">&#39;lr&#39;</span>: <span class="fl">1e-5</span>},  </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&#39;params&#39;</span>: model.classifier.parameters(), <span class="st">&#39;lr&#39;</span>: <span class="fl">1e-3</span>}</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[0 1 2 3] [2456 2546 2513 2485]
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_transform.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.weight&#39;]
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;pre_classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
</div>
</div>
<div id="J4y8mmX9bloZ" class="cell code" id="J4y8mmX9bloZ">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO split the sample into a training and a test set </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(df[<span class="st">&#39;text&#39;</span>].tolist(), df[<span class="st">&#39;label_int&#39;</span>].tolist(), test_size<span class="op">=</span><span class="fl">.2</span>, stratify<span class="op">=</span>df[<span class="st">&#39;label_int&#39;</span>])</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># generate batches</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the number of batches based on the batch size</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>num_train_batches <span class="op">=</span> <span class="bu">len</span>(X_train) <span class="op">//</span> batch_size</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>num_test_batches <span class="op">=</span> <span class="bu">len</span>(X_test) <span class="op">//</span> batch_size</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># trim the data to fit the batch size</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train[:num_train_batches <span class="op">*</span> batch_size]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test[:num_test_batches <span class="op">*</span> batch_size]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train[:num_train_batches <span class="op">*</span> batch_size]</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test[:num_test_batches <span class="op">*</span> batch_size]</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the lists to numpy arrays</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array(X_train)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array(X_test)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array(y_train)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.array(y_test)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># reshape the data into batches</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train.reshape(<span class="op">-</span><span class="dv">1</span>, batch_size)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test.reshape(<span class="op">-</span><span class="dv">1</span>, batch_size)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train.reshape(<span class="op">-</span><span class="dv">1</span>, batch_size)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test.reshape(<span class="op">-</span><span class="dv">1</span>, batch_size)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>X_train, X_test <span class="op">=</span> X_train.tolist(), X_test.tolist()</span></code></pre></div>
</div>
<div id="piano-compound" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="piano-compound"
data-outputId="03af565e-0d7e-427e-8314-7fbf71b4918f">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO fit the model and print the obtained accuracy (hint: you can follow the training steps in the notebook. To learn more, checkout the trainer class of huggingface transformers)</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># train</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text, labels <span class="kw">in</span> tqdm(<span class="bu">zip</span>(X_train, y_train), total<span class="op">=</span><span class="bu">len</span>(X_train)):</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prepare model input through our tokenizer</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        model_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># place everything on the right device</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        model_inputs <span class="op">=</span> {k:v.to(device) <span class="cf">for</span> k,v <span class="kw">in</span> model_inputs.items()}</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels have to be torch long tensors</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.tensor(labels).<span class="bu">long</span>().to(device)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># now, we can perform the forward pass</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="op">**</span>model_inputs, labels<span class="op">=</span>labels)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        loss, logits <span class="op">=</span> output[:<span class="dv">2</span>]</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and the backward pass</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 1000/1000 [04:22&lt;00:00,  3.80it/s]
</code></pre>
</div>
</div>
<div id="ectMsFHdrTOX" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="ectMsFHdrTOX" data-outputId="4c517850-07b0-4302-f9d3-b7941d41e3e5">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>predictions, targets <span class="op">=</span> [], []</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text, labels <span class="kw">in</span> tqdm(<span class="bu">zip</span>(X_test, y_test), total<span class="op">=</span><span class="bu">len</span>(X_test)):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        model_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">&quot;pt&quot;</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        model_inputs <span class="op">=</span> {k:v.to(device) <span class="cf">for</span> k,v <span class="kw">in</span> model_inputs.items()}</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(<span class="op">**</span>model_inputs)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> output[<span class="dv">0</span>]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># prediction is the argmax of the logits</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        predictions.extend(logits.argmax(dim<span class="op">=</span><span class="dv">1</span>).tolist())</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        targets.extend(labels)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> metrics.accuracy_score(targets, predictions)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;accuracy&quot;</span>, accuracy)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>classification_report <span class="op">=</span> metrics.classification_report(targets, predictions)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (classification_report)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 250/250 [00:21&lt;00:00, 11.88it/s]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>accuracy 0.908
              precision    recall  f1-score   support

           0       0.95      0.98      0.97       491
           1       0.88      0.91      0.89       509
           2       0.90      0.91      0.91       503
           3       0.90      0.83      0.87       497

    accuracy                           0.91      2000
   macro avg       0.91      0.91      0.91      2000
weighted avg       0.91      0.91      0.91      2000

</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<div id="e7fe3a17" class="cell markdown" id="e7fe3a17">
<h1 id="doc-embedding">Doc Embedding</h1>
</div>
<div id="3b41f71c" class="cell code" id="3b41f71c">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain the data</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget http:<span class="op">//</span>alt.qcri.org<span class="op">/</span>semeval2017<span class="op">/</span>task1<span class="op">/</span>data<span class="op">/</span>uploads<span class="op">/</span>sts2017.<span class="bu">eval</span>.v1<span class="fl">.1</span>.<span class="bu">zip</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget http:<span class="op">//</span>alt.qcri.org<span class="op">/</span>semeval2017<span class="op">/</span>task1<span class="op">/</span>data<span class="op">/</span>uploads<span class="op">/</span>sts2017.gs.<span class="bu">zip</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip sts2017.<span class="bu">eval</span>.v1<span class="fl">.1</span>.<span class="bu">zip</span> </span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip sts2017.gs.<span class="bu">zip</span> </span></code></pre></div>
</div>
<div id="d48a11ab" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="d48a11ab" data-outputId="9b2b19e2-72e2-4648-a3ca-c3972c82a693">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_STS_data():</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;STS2017.gs/STS.gs.track5.en-en.txt&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="bu">float</span>(line.strip()) <span class="cf">for</span> line <span class="kw">in</span> f]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    text_a, text_b <span class="op">=</span> [], []</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;STS2017.eval.v1.1/STS.input.track5.en-en.txt&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>            line <span class="op">=</span> line.strip().split(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            text_a.append(line[<span class="dv">0</span>])</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>            text_b.append(line[<span class="dv">1</span>])</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text_a, text_b, labels</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>text_a, text_b, labels <span class="op">=</span> load_STS_data()</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>text_a[<span class="dv">0</span>], text_b[<span class="dv">0</span>], labels[<span class="dv">0</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="43">
<pre><code>(&#39;A person is on a baseball team.&#39;,
 &#39;A person is playing basketball on a team.&#39;,
 2.4)</code></pre>
</div>
</div>
<div id="dee8bcb4" class="cell code" id="dee8bcb4">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># some utils</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> spearmanr</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(predictions, labels):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;spearman&#39;s rank correlation&quot;</span>, spearmanr(predictions, labels)[<span class="dv">0</span>])</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> dot</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cosine_similarity(a,b):</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dot(a, b)<span class="op">/</span>(norm(a)<span class="op">*</span>norm(b))</span></code></pre></div>
</div>
<div id="46f02f97" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="46f02f97" data-outputId="f0505c90-5bc3-4248-9747-3b209b95308f">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Wordcounts baseline</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> CountVectorizer()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>vec.fit(text_a <span class="op">+</span> text_b)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># encode documents</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>text_a_encoded <span class="op">=</span> np.array(vec.transform(text_a).todense())</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>text_b_encoded <span class="op">=</span> np.array(vec.transform(text_b).todense())</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># predict cosine similarities</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> [cosine_similarity(a,b) <span class="cf">for</span> a,b <span class="kw">in</span> <span class="bu">zip</span>(text_a_encoded, text_b_encoded)]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>evaluate(predictions, labels)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>spearman&#39;s rank correlation 0.6998056665685976
</code></pre>
</div>
</div>
<div id="965242a5" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="965242a5" data-outputId="a0d74937-5499-4ee7-e8d7-28ca07837d83">
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO train Doc2Vec on the texts in the dataset </span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&#39;punkt&#39;</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.doc2vec <span class="im">import</span> Doc2Vec, TaggedDocument</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> random <span class="im">import</span> shuffle</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> text_a <span class="op">+</span> text_b</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># tokenize</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>docs_a <span class="op">=</span> []</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> text_a:</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    docs_a <span class="op">+=</span> [word_tokenize(text)]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>docs_b <span class="op">=</span> []</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> text_b:</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    docs_b <span class="op">+=</span> [word_tokenize(text)]</span></code></pre></div>
<div class="output stream stderr">
<pre><code>[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</code></pre>
</div>
</div>
<div id="2PkwHOLd2mpZ" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="2PkwHOLd2mpZ" data-outputId="975ac178-963d-4990-ce79-6c96972d4f7e">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Doc2Vec (a)</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>doc_iterator_a <span class="op">=</span> [TaggedDocument(doc, [i]) <span class="cf">for</span> i, doc <span class="kw">in</span> <span class="bu">enumerate</span>(docs_a)]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>d2v_a <span class="op">=</span> Doc2Vec(doc_iterator_a,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>                min_count<span class="op">=</span><span class="dv">10</span>, <span class="co"># minimum word count</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>                window<span class="op">=</span><span class="dv">10</span>,    <span class="co"># window size</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                vector_size<span class="op">=</span><span class="dv">100</span>, <span class="co"># size of document vector</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>                sample<span class="op">=</span><span class="fl">1e-4</span>, </span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>                negative<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>                workers<span class="op">=</span><span class="dv">4</span>, <span class="co"># threads</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>                max_vocab_size<span class="op">=</span><span class="dv">1000</span>) <span class="co"># max vocab size</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>doc_iterator_b <span class="op">=</span> [TaggedDocument(doc, [i]) <span class="cf">for</span> i, doc <span class="kw">in</span> <span class="bu">enumerate</span>(docs_b)]</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>d2v_b <span class="op">=</span> Doc2Vec(doc_iterator_b,</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>                min_count<span class="op">=</span><span class="dv">10</span>, <span class="co"># minimum word count</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>                window<span class="op">=</span><span class="dv">10</span>,    <span class="co"># window size</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>                vector_size<span class="op">=</span><span class="dv">100</span>, <span class="co"># size of document vector</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>                sample<span class="op">=</span><span class="fl">1e-4</span>, </span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>                negative<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>                workers<span class="op">=</span><span class="dv">4</span>, <span class="co"># threads</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>                max_vocab_size<span class="op">=</span><span class="dv">1000</span>) <span class="co"># max vocab size              </span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="co"># matrix of all document vectors:</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>D_a <span class="op">=</span> d2v_a.dv.vectors</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>D_b <span class="op">=</span> d2v_b.dv.vectors</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>D_a.shape, D_b.shape </span></code></pre></div>
<div class="output execute_result" data-execution_count="126">
<pre><code>((250, 100), (250, 100))</code></pre>
</div>
</div>
<div id="_wb-qNSDwcih" class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="_wb-qNSDwcih" data-outputId="15d2131a-de17-4183-96bd-55f342a9c999">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO derive the word vectors for each text in the dataset </span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>vectors_a <span class="op">=</span> [d2v_a.infer_vector(text_a)]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>vectors_b <span class="op">=</span> [d2v_b.infer_vector(text_b)]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO compute cosine similarity between the text pairs and evaluate spearman&#39;s rank correlation</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>similarity_matrix <span class="op">=</span> cosine_similarity(vectors_a[<span class="dv">0</span>], vectors_b[<span class="dv">0</span>])</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similarity_matrix)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Don&#39;t worry if results are not satisfactory using Doc2Vec (the dataset is too small to train good embeddings)</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>0.0033083113
</code></pre>
</div>
</div>
<div id="e67b67c8" class="cell code" id="e67b67c8">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO do the same with embeddings provided by spaCy</span></span></code></pre></div>
</div>
<div id="2cf40ced" class="cell code" id="2cf40ced">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">##TODO do the same with SBERT embeddings</span></span></code></pre></div>
</div>
</body>
</html>
