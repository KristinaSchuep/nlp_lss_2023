{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d21e1ead",
      "metadata": {
        "id": "d21e1ead"
      },
      "source": [
        "# HW04: ML and DL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680d1f0b",
      "metadata": {
        "id": "680d1f0b"
      },
      "source": [
        "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bf38c8",
      "metadata": {
        "id": "c9bf38c8"
      },
      "source": [
        "## Load and Pre-process Text\n",
        "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21439804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21439804",
        "outputId": "a52a81da-67c2-47c6-8185-1dfb37a3f3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-12 04:42:04--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4029756 (3.8M) [application/x-gzip]\n",
            "Saving to: ‘scale_data.tar.gz’\n",
            "\n",
            "scale_data.tar.gz   100%[===================>]   3.84M  13.5MB/s    in 0.3s    \n",
            "\n",
            "2023-03-12 04:42:04 (13.5 MB/s) - ‘scale_data.tar.gz’ saved [4029756/4029756]\n",
            "\n",
            "--2023-03-12 04:42:04--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8853204 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘scale_whole_review.tar.gz’\n",
            "\n",
            "scale_whole_review. 100%[===================>]   8.44M  23.7MB/s    in 0.4s    \n",
            "\n",
            "2023-03-12 04:42:05 (23.7 MB/s) - ‘scale_whole_review.tar.gz’ saved [8853204/8853204]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# In this tutorial, we do sentiment analysis\n",
        "# download the data\n",
        "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "#!tar xf aclImdb_v1.tar.gz\n",
        "\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
        " \n",
        "!tar xf scale_data.tar.gz \n",
        "!tar xf scale_whole_review.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d685ef2e",
      "metadata": {
        "id": "d685ef2e"
      },
      "source": [
        "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18a238d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18a238d",
        "outputId": "d3821540-d44d-4c8e-90a4-eed10492cba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: stanley ipkiss whose letter to the local paper signed nice guys finish last had generated torrent of replies the year before has been undergoing change lately bank clerk ipkiss played with sweet sincerity by jim carrey discovers mask that like dr jekyll potion temporarily creates an all new person to understand how the mask works he turns to masks that people wear expert named dr neuman played with dripping sincerity and dead pan humor by ben stein although the doctor proves useless stanley finally discovers for himself what the mask does it magnifies your inner desires since ipkiss is an incurable romantic who spends his free time watching cartoons it is inevitable that the mask turns him into the world greatest lover and song and dance man after avoiding carrey for years was blown away by his performance in liar liar one of this year funniest films since the mask in was the movie that really launched his film career suggested we check it out one evening on vacation with the help of realistic and colorful special effects carrey as the mask struts his stuff non stop when he meets his heart throb cameron diaz in her film debut as blond bombshell tina carlyle at nightclub his heart jumps out of his body and his jaw drops open far enough for yard long tongue to drool out carrey shows off his ability to impersonate countless other actors and reenact their most famous scenes when trapped by bad guys with machine guns he pulls out two cartoonish cannon like guns with dozen barrels each you have to ask yourself question he warns with soft clint eastwood voice do feel lucky ha nguyen stream of elaborate costumes for the mask sets the tone for all of the mask emotions when the mask is trapped by an army of police he switches to latin costume and soon has everyone formed into singing and dancing conga line in highly imaginative film the only surprise is how slowly director chuck russell paces the non mask scenes although it never got the belly laughs out of me that liar liar did the mask delivers some well choreographed numbers and displays carrey talents well still must confess that my favorite character in the film was not stanley but milo his little pooch why cute animals like max who plays milo do not get more acting roles in the movies remains mystery the mask runs it is rated pg for some cartoonish violence and some profanity most of it is so mild that the film should be fine for kids around and up my son jeffrey age thought the show was really good and funny recommend the picture to you and give it must see film excellent show look for it average movie kind of enjoyable poor show don waste your money totally and painfully unbearable picture review written on july opinions expressed are mine and not meant to reflect my employer \n",
            "label: 0.7\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from gensim.utils import simple_preprocess\n",
        "def load_data():\n",
        "    examples, labels = [], []\n",
        "    authors = os.listdir(\"scale_whole_review\")\n",
        "    for author in authors:\n",
        "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
        "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
        "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
        "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
        "            for idx, rating in zip(ids, ratings):\n",
        "                labels.append(float(rating.strip()))\n",
        "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
        "                with open(filename_text, encoding='latin-1') as f:\n",
        "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
        "    return examples, labels\n",
        "                  \n",
        "X,y  = load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "print (\"text:\", X_train[0], \"\\nlabel:\", y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284033cf",
      "metadata": {
        "id": "284033cf"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09aff185",
      "metadata": {
        "id": "09aff185"
      },
      "outputs": [],
      "source": [
        "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
        "                        max_df=.5,  \n",
        "                        stop_words='english',\n",
        "                        ngram_range=(1,2))\n",
        "\n",
        "##TODO train vectorizer\n",
        "vec.fit(X_train)\n",
        "##TODO transform X_train to TF-IDF values\n",
        "X_train_tfidf = vec.transform(X_train)\n",
        "##TODO transform X_test to TF-IDF values\n",
        "X_test_tfidf = vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d44dbb",
      "metadata": {
        "id": "58d44dbb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "##TODO scale both training and test data with the standard scaler\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "X_test_scaled = scaler.transform(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9d8a57",
      "metadata": {
        "id": "ad9d8a57"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5f4520",
      "metadata": {
        "id": "1e5f4520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36677645-49ac-4d68-8205-72c79b46ce80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse 0.01512374023090484\n",
            "r2 0.5067135227411286\n"
          ]
        }
      ],
      "source": [
        "##TODO train an elastic net on the transformed output of the scaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "en = ElasticNet(alpha=0.01)\n",
        "\n",
        "##TODO train the ElasticNet\n",
        "en.fit(X_train_scaled, y_train)\n",
        "##TODO predict the testset\n",
        "predicted = en.predict(X_test_scaled)\n",
        "\n",
        "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
        "##TODO print mean squared error and r2 score on the test set\n",
        "print (\"mse\", mean_squared_error(y_test, predicted))\n",
        "print (\"r2\", r2_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872d1ef8",
      "metadata": {
        "id": "872d1ef8"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e2756e",
      "metadata": {
        "id": "27e2756e"
      },
      "source": [
        "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbd752c",
      "metadata": {
        "id": "9cbd752c"
      },
      "outputs": [],
      "source": [
        "y_train = [1 if i >= 0.5 else 0 for i in y_train]\n",
        "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2c239d",
      "metadata": {
        "id": "2c2c239d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ad2d68-8214-46f7-d2c9-1c4f2e40d26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8202179176755447\n",
            "(1, 5616)\n",
            "0.17038391559368846 brilliance\n",
            "0.17184606388124232 engaging\n",
            "0.1754505048684661 area\n",
            "0.1780437794159087 mysteries\n",
            "0.17826925287087944 imaginative\n",
            "0.18180467904844047 destruction\n",
            "0.1822993821958528 effective\n",
            "0.1828440074888503 surprisingly\n",
            "0.18647911705118803 fine\n",
            "0.20526358328144687 success\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "##TODO train logistic regression \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "##TODO train a logistic regression on the scaled X_train\n",
        "lr_clf = logistic_regression.fit(X_train_scaled, y_train)\n",
        "##TODO predict the testset \n",
        "predicted = lr_clf.predict(X_test_scaled)\n",
        "\n",
        "##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
        "def map_predictions(predicted):\n",
        "    predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
        "    return predicted\n",
        "\n",
        "##TODO print the accuracy of our classifier on the testset\n",
        "binary_predictions = map_predictions(predicted)\n",
        "print (accuracy_score(y_test, binary_predictions))\n",
        "## TODO print the 10 most informative words of the regression (the 10 words having the highest coefficients)\n",
        "import numpy as np\n",
        "id2word = vec.get_feature_names_out()\n",
        "coefs = logistic_regression.coef_.squeeze()\n",
        "indices = np.argsort(coefs)\n",
        "print (logistic_regression.coef_.shape)\n",
        "for i in indices[-10:]:\n",
        "    print (coefs[i], id2word[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "3hNKx6fUGgCL"
      },
      "id": "3hNKx6fUGgCL"
    },
    {
      "cell_type": "markdown",
      "id": "d0a6bc62",
      "metadata": {
        "id": "d0a6bc62"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "n1kAYU0SSpAH",
        "outputId": "094eed91-4bdd-46ef-e2c3-1fff7a275386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-16 04:14:03--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29470338 (28M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  28.10M   148MB/s    in 0.2s    \n",
            "\n",
            "2023-03-16 04:14:03 (148 MB/s) - ‘train.csv’ saved [29470338/29470338]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           label                                              title  \\\n",
              "103993  business  Easymobile Closer to  #39;Lean, Low-Cost Servi...   \n",
              "5164       world                     Bombs explode in Nepal capital   \n",
              "54374   sci/tech  Update: AMD's Q3 led by strong Opteron, Athlon...   \n",
              "107591     sport                           Blue Jays sign Menechino   \n",
              "32652      sport  Rallying: Sparkling Solberg win means title wa...   \n",
              "\n",
              "                                                     lead  \\\n",
              "103993  Moves by easyJet founder Stelios Haji-Ioannou ...   \n",
              "5164    KATHMANDU (Reuters) - Nepal #39;s embattled go...   \n",
              "54374   As expected, Advanced Micro Devices Inc.'s (AM...   \n",
              "107591  Toronto, ON (Sports Network) - The Toronto Blu...   \n",
              "32652   It may not be enough to tilt the championship ...   \n",
              "\n",
              "                                                     text  \n",
              "103993  Easymobile Closer to  #39;Lean, Low-Cost Servi...  \n",
              "5164    Bombs explode in Nepal capital KATHMANDU (Reut...  \n",
              "54374   Update: AMD's Q3 led by strong Opteron, Athlon...  \n",
              "107591  Blue Jays sign Menechino Toronto, ON (Sports N...  \n",
              "32652   Rallying: Sparkling Solberg win means title wa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb15c957-a789-4835-a3d4-986a8cbb9788\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>lead</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>103993</th>\n",
              "      <td>business</td>\n",
              "      <td>Easymobile Closer to  #39;Lean, Low-Cost Servi...</td>\n",
              "      <td>Moves by easyJet founder Stelios Haji-Ioannou ...</td>\n",
              "      <td>Easymobile Closer to  #39;Lean, Low-Cost Servi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5164</th>\n",
              "      <td>world</td>\n",
              "      <td>Bombs explode in Nepal capital</td>\n",
              "      <td>KATHMANDU (Reuters) - Nepal #39;s embattled go...</td>\n",
              "      <td>Bombs explode in Nepal capital KATHMANDU (Reut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54374</th>\n",
              "      <td>sci/tech</td>\n",
              "      <td>Update: AMD's Q3 led by strong Opteron, Athlon...</td>\n",
              "      <td>As expected, Advanced Micro Devices Inc.'s (AM...</td>\n",
              "      <td>Update: AMD's Q3 led by strong Opteron, Athlon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107591</th>\n",
              "      <td>sport</td>\n",
              "      <td>Blue Jays sign Menechino</td>\n",
              "      <td>Toronto, ON (Sports Network) - The Toronto Blu...</td>\n",
              "      <td>Blue Jays sign Menechino Toronto, ON (Sports N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32652</th>\n",
              "      <td>sport</td>\n",
              "      <td>Rallying: Sparkling Solberg win means title wa...</td>\n",
              "      <td>It may not be enough to tilt the championship ...</td>\n",
              "      <td>Rallying: Sparkling Solberg win means title wa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb15c957-a789-4835-a3d4-986a8cbb9788')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb15c957-a789-4835-a3d4-986a8cbb9788 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb15c957-a789-4835-a3d4-986a8cbb9788');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Import the AG news dataset (same as hw01)\n",
        "#Download them from here \n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
        "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
        "df = df.sample(n=10000) # # only use 10K datapoints\n",
        "df.head()"
      ],
      "id": "n1kAYU0SSpAH"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new variable \"business\" that takes value 1 if the label is business and 0 otherwise\n",
        "df['business'] = df['label'].apply(lambda x: int(x=='business'))\n",
        "y = df['business'].values\n",
        "df['business'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-fUUrUNSui1",
        "outputId": "52ad5192-6c83-42e9-b079-650822d8cac7"
      },
      "id": "L-fUUrUNSui1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103993    1\n",
              "5164      0\n",
              "54374     0\n",
              "107591    0\n",
              "32652     0\n",
              "Name: business, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "##pre-process text as you did in HW02\n",
        "def tokenize(x):\n",
        "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
        "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize(x))\n",
        "df[\"preprocessed\"] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "##TODO vectorize the pre-processed text using CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
        "                        max_df=.9,  \n",
        "                        max_features=1000,\n",
        "                        stop_words='english',\n",
        "                        ngram_range=(1,3))\n",
        "X = vectorizer.fit_transform(df['preprocessed'])\n",
        "pd.to_pickle(X,'X.pkl')"
      ],
      "metadata": {
        "id": "jI1Fgc42SwQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2028710b-f083-4305-c52e-d279d805e82a"
      },
      "id": "jI1Fgc42SwQb",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"preprocessed\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FjtUoOhWjNZ",
        "outputId": "b3c8a05e-b70e-4655-ecbc-da2c9ad69a77"
      },
      "id": "4FjtUoOhWjNZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103993    easymobile close   39;lean low cost service la...\n",
              "5164      bomb explode nepal capital kathmandu reuters n...\n",
              "54374     update amd q3 lead strong opteron athlon sale ...\n",
              "107591    blue jays sign menechino toronto sports networ...\n",
              "32652     rallying sparkling solberg win mean title wait...\n",
              "                                ...                        \n",
              "57581     nuclear asset vanish iraq technology help prod...\n",
              "23434     swede win date hewitt lleyton hewitt suppose p...\n",
              "87698     pire feel wrath france national coach rarely p...\n",
              "79756     union leader weigh late contract offer union l...\n",
              "99454     henson play cowboys start henson wait like fin...\n",
              "Name: preprocessed, Length: 10000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6e66fc",
      "metadata": {
        "id": "9b6e66fc"
      },
      "source": [
        "Your goal here is to use features from the Vectorized text to predict whether the snippet is from a business article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0b718ae5",
      "metadata": {
        "id": "0b718ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29958062-8c65-403d-e17c-573ee3dbd11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 50]          21,050\n",
            "              ReLU-2                   [-1, 50]               0\n",
            "            Linear-3                   [-1, 50]           2,550\n",
            "              ReLU-4                   [-1, 50]               0\n",
            "           Dropout-5                   [-1, 50]               0\n",
            "            Linear-6                   [-1, 50]           2,550\n",
            "              ReLU-7                   [-1, 50]               0\n",
            "           Dropout-8                   [-1, 50]               0\n",
            "            Linear-9                    [-1, 1]              51\n",
            "          Sigmoid-10                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 26,201\n",
            "Trainable params: 26,201\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.10\n",
            "Estimated Total Size (MB): 0.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "\n",
        "## TODO build a MLP model with at least 2 hidden layers with ReLU activation, followed by dropout and an output layer with sigmoid activation\n",
        "input_dim = X.shape[1]\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        # use nn.Sequential to sequentially stack modules\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 50), # input layer\n",
        "            nn.ReLU(), # activation function\n",
        "            nn.Linear(50, 50), # hidden layer 1\n",
        "            nn.ReLU(), # activation 1\n",
        "            nn.Dropout(0.5), # dropout 1\n",
        "            nn.Linear(50, 50), # hidden layer 2\n",
        "            nn.ReLU(), # activation 2\n",
        "            nn.Dropout(0.5), # dropout 2\n",
        "            nn.Linear(50, 1), # output layer\n",
        "            nn.Sigmoid(), # sigmoid\n",
        "        )\n",
        "        \n",
        "    # define the forward propagation which is necessary for torch models\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "## TODO summarize the model using torchsummary\n",
        "model_summarize = MLP()\n",
        "\n",
        "summary(model_summarize, input_size=(input_dim,))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO fit the model using early stopping (patience = 0.5) to predict the business label (split can follow train:valid:test = 8:1:1)\n",
        "# (hint: early stopping means if the validation score does not increase for more than \"patience\" times, training should stop and load the best model so far)\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# First prepare the datasets\n",
        "class GenericDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "tsize = math.ceil(0.1 * len(y))\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X.toarray(), np.array(y), test_size=tsize)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=tsize)\n",
        "\n",
        "train_dataset = GenericDataset(X_train, y_train)\n",
        "valid_dataset = GenericDataset(X_valid, y_valid)\n",
        "test_dataset = GenericDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "mean_train_losses = []\n",
        "mean_valid_losses = []\n",
        "valid_acc_list = []\n",
        "patience = 0\n",
        "epochs = 100\n",
        "best_score = 0\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "model = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss() \n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # switch the model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for i, (Xs, labels) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(Xs.float().to(device))\n",
        "        loss = loss_fn(outputs, labels.float().unsqueeze(1).to(device)) # shape (32,) -> (32,1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "            \n",
        "    model.eval()\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for i, (Xs, labels) in enumerate(valid_loader):\n",
        "            outputs = model(Xs.float().to(device))\n",
        "            loss = loss_fn(outputs, labels.float().unsqueeze(1).to(device))\n",
        "            \n",
        "            valid_losses.append(loss.item())\n",
        "            \n",
        "            predicted = [1 if d > 0.5 else 0 for d in outputs.data.squeeze()]\n",
        "            pred_labels.extend(predicted)\n",
        "            true_labels.extend(list(labels))\n",
        "            \n",
        "    mean_train_losses.append(np.mean(train_losses))\n",
        "    mean_valid_losses.append(np.mean(valid_losses))\n",
        "    \n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    if accuracy > best_score:\n",
        "      torch.save(model, 'best.pt')\n",
        "      best_score = accuracy\n",
        "      patience = 0 # reset patience\n",
        "    else:\n",
        "      patience += 1\n",
        "    valid_acc_list.append(accuracy)\n",
        "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, valid acc : {:.2f}%, patience: {}'\\\n",
        "         .format(epoch+1, np.mean(train_losses), np.mean(valid_losses), accuracy, patience))\n",
        "    if patience > 5:\n",
        "      print('epoch : {}, patience : {}, training early stops'.format(epoch+1, patience))\n",
        "      break\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e72DJSgsgSr1",
        "outputId": "0b5bdff3-e9ff-4981-a8e3-5e0e9843733c"
      },
      "id": "e72DJSgsgSr1",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1, train loss : 0.4137, valid loss : 0.2714, valid acc : 0.89%, patience: 0\n",
            "epoch : 2, train loss : 0.2773, valid loss : 0.2663, valid acc : 0.89%, patience: 1\n",
            "epoch : 3, train loss : 0.2512, valid loss : 0.2591, valid acc : 0.90%, patience: 0\n",
            "epoch : 4, train loss : 0.2236, valid loss : 0.2667, valid acc : 0.90%, patience: 1\n",
            "epoch : 5, train loss : 0.1991, valid loss : 0.2892, valid acc : 0.89%, patience: 2\n",
            "epoch : 6, train loss : 0.1617, valid loss : 0.3098, valid acc : 0.89%, patience: 3\n",
            "epoch : 7, train loss : 0.1309, valid loss : 0.3392, valid acc : 0.88%, patience: 4\n",
            "epoch : 8, train loss : 0.0962, valid loss : 0.3790, valid acc : 0.90%, patience: 5\n",
            "epoch : 9, train loss : 0.0734, valid loss : 0.4282, valid acc : 0.89%, patience: 6\n",
            "epoch : 9, patience : 6, training early stops\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}