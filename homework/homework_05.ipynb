{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbe9d25b",
      "metadata": {
        "id": "dbe9d25b"
      },
      "source": [
        "# HW05: Word Embeddings\n",
        "\n",
        "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3a0596",
      "metadata": {
        "id": "8b3a0596"
      },
      "source": [
        "**Essay Feedback**\n",
        "\n",
        "Please provide feedback to two classmates' essays on Eduflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea14794",
      "metadata": {
        "id": "5ea14794"
      },
      "source": [
        "**Training word2vec**\n",
        "\n",
        "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a38d6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a38d6e",
        "outputId": "120e6961-62a6-4074-ad5b-ed48820d4dbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_records': 1701,\n",
              " 'record_format': 'list of str (tokens)',\n",
              " 'file_size': 33182058,\n",
              " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              " 'license': 'not found',\n",
              " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              " 'file_name': 'text8.gz',\n",
              " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              " 'parts': 1}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "api.info(\"text8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a49444c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a49444c",
        "outputId": "16b4d445-c29b-455f-95b8-2c5c6b26beca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "dataset = api.load(\"text8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61fa38b5",
      "metadata": {
        "id": "61fa38b5"
      },
      "outputs": [],
      "source": [
        "##TODO train a word2vec model on this dataset which appear at least 10 times in the corpus\n",
        "\n",
        "# train the model\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(dataset,  \n",
        "               workers = 8, # Number of threads to run in parallel\n",
        "               vector_size=300,  # Word vector dimensionality     \n",
        "               min_count =  10, # Minimum word count  \n",
        "               window = 5, # Context window size      \n",
        "               sample = 1e-3, # Downsample setting for frequent words\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "pm2YeCFh4mQH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm2YeCFh4mQH",
        "outputId": "b79c9da3-cf5c-4c77-e6f8-02d5eee20b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "XC4IZFrs3H9J",
      "metadata": {
        "id": "XC4IZFrs3H9J"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_save_name = 'w2v-vectors.pkl'\n",
        "path = F\"/content/gdrive/My Drive/Colab Notebooks/Homework/{model_save_name}\" \n",
        "model.save(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rfuixPYM7ptJ",
      "metadata": {
        "id": "rfuixPYM7ptJ"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "from gensim.models import Word2Vec\n",
        "model_save_name = 'w2v-vectors.pkl'\n",
        "path = F\"/content/gdrive/My Drive/Colab Notebooks/Homework/{model_save_name}\" \n",
        "model = Word2Vec.load(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af69360",
      "metadata": {
        "id": "4af69360"
      },
      "source": [
        "**Word Similarities**\n",
        "\n",
        "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf99280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf99280",
        "outputId": "ad2ac08b-8d37-41f1-e5ed-35aec79ca306"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('kings', 0.6537180542945862),\n",
              " ('prince', 0.6409084796905518),\n",
              " ('queen', 0.6376361846923828),\n",
              " ('throne', 0.6341038942337036),\n",
              " ('sultan', 0.6066715717315674),\n",
              " ('aragon', 0.6039237380027771),\n",
              " ('darius', 0.5958287715911865),\n",
              " ('emperor', 0.594192624092102),\n",
              " ('duke', 0.5894923210144043),\n",
              " ('vii', 0.5867447853088379)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.wv\n",
        "word_vectors  = model.wv\n",
        "\n",
        "##TODO find the closest words to king\n",
        "result = word_vectors.most_similar('king')\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c30c847",
      "metadata": {
        "id": "9c30c847"
      },
      "source": [
        "King is to man as woman is to X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615b6116",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "615b6116",
        "outputId": "25418cf0-77f5-4be8-f1fa-b299c81e279e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('queen', 0.6237307190895081)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
        "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "result[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af37627",
      "metadata": {
        "id": "2af37627"
      },
      "source": [
        "**Evaluate Word Similarities** \n",
        "\n",
        "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71515b20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71515b20",
        "outputId": "6a76aa68-0a05-4419-a5f4-56d41bc38739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-30 09:57:03--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
            "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
            "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5460 (5.3K) [application/x-gzip]\n",
            "Saving to: ‘ws353simrel.tar.gz’\n",
            "\n",
            "ws353simrel.tar.gz  100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-30 09:57:03 (550 MB/s) - ‘ws353simrel.tar.gz’ saved [5460/5460]\n",
            "\n",
            "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
          ]
        }
      ],
      "source": [
        "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
        "!tar xf ws353simrel.tar.gz\n",
        "\n",
        "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
        "\n",
        "def load_data(path):\n",
        "    X, y = [], []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
        "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
        "    return X, y\n",
        "\n",
        "X, y = load_data(path)\n",
        "print (X[:3], y[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8ced33",
      "metadata": {
        "id": "9c8ced33"
      },
      "outputs": [],
      "source": [
        "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
        "# if a word is not present in our model, we assign similarity 0 for the respective text pair\n",
        "\n",
        "similarity = []\n",
        "\n",
        "for pair in X:\n",
        "  # extract pair\n",
        "  first_word, second_word = pair\n",
        "\n",
        "  # check if word is present in our model\n",
        "  if ((first_word in model.wv.index_to_key) and (second_word in model.wv.index_to_key)):\n",
        "    # compute similarity\n",
        "    sim = model.wv.similarity(first_word, second_word)\n",
        "    similarity.append(sim)\n",
        "  else:\n",
        "    # assign 0 if one of the words is not in vocabulary\n",
        "    similarity.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kv2KN03H0yPd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv2KN03H0yPd",
        "outputId": "5e14a841-e361-48f6-85b7-0d5ca1894d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pair: ('tiger', 'cat')  ---  similarity from WordSim353: 7.35   --- Similarity from word2vec: 0.5433863\n",
            "pair: ('tiger', 'tiger')  ---  similarity from WordSim353: 10.0   --- Similarity from word2vec: 1.0\n",
            "pair: ('plane', 'car')  ---  similarity from WordSim353: 5.77   --- Similarity from word2vec: 0.45190403\n",
            "pair: ('train', 'car')  ---  similarity from WordSim353: 6.31   --- Similarity from word2vec: 0.53525\n",
            "pair: ('television', 'radio')  ---  similarity from WordSim353: 6.77   --- Similarity from word2vec: 0.66451186\n",
            "pair: ('media', 'radio')  ---  similarity from WordSim353: 7.42   --- Similarity from word2vec: 0.3964206\n",
            "pair: ('bread', 'butter')  ---  similarity from WordSim353: 6.19   --- Similarity from word2vec: 0.68906647\n",
            "pair: ('cucumber', 'potato')  ---  similarity from WordSim353: 5.92   --- Similarity from word2vec: 0.6796025\n",
            "pair: ('doctor', 'nurse')  ---  similarity from WordSim353: 7.0   --- Similarity from word2vec: 0.49071032\n",
            "pair: ('professor', 'doctor')  ---  similarity from WordSim353: 6.62   --- Similarity from word2vec: 0.41851917\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,10):\n",
        "  print(\"pair:\", X[i], \" ---  similarity from WordSim353:\", y[i], \"  --- Similarity from word2vec:\", similarity[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd47f93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebd47f93",
        "outputId": "be2f46a4-66ca-49a4-f86c-f1507e017d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation:  0.6481808256998792\n",
            "p-value:  1.4102608500152953e-25\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
        "res = spearmanr(similarity, y)\n",
        "print('Correlation: ', res.statistic)\n",
        "print('p-value: ',res.pvalue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec86899",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ec86899",
        "outputId": "4a33ad13-25ed-41d9-8ade-efbf261d1735"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-558bbaa0b4dd>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return token1.similarity(token2)\n",
            "<ipython-input-21-558bbaa0b4dd>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  token1 = en(word1)\n",
            "<ipython-input-21-558bbaa0b4dd>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return token1.similarity(token2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation:  0.09174883124982042\n",
            "p-value:  0.19295227692420674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-558bbaa0b4dd>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  token2 = en(word2)\n",
            "<ipython-input-21-558bbaa0b4dd>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return token1.similarity(token2)\n",
            "<ipython-input-21-558bbaa0b4dd>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  token2 = en(word2)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "\n",
        "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
        "def get_similarity(word1, word2):\n",
        "    token1 = en(word1)\n",
        "    token2 = en(word2)\n",
        "    return token1.similarity(token2)\n",
        "\n",
        "similarity2 = []\n",
        "for pair in X:\n",
        "  first_word, second_word = pair\n",
        "  sim = get_similarity(first_word, second_word)\n",
        "  similarity2.append(sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7Jt052_bLNY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Jt052_bLNY",
        "outputId": "45d54ba5-7893-45da-fdf8-cd74384e9c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation:  0.09174883124982042\n",
            "p-value:  0.19295227692420674\n"
          ]
        }
      ],
      "source": [
        "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
        "# Don't worry if results are not too convincing for this experiment\n",
        "res = spearmanr(similarity2, y)\n",
        "print('Correlation: ', res.statistic)\n",
        "print('p-value: ', res.pvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29de774",
      "metadata": {
        "id": "d29de774"
      },
      "source": [
        "**PyTorch Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3927e048",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:42:21.281177Z",
          "start_time": "2022-03-22T21:42:21.208787Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3927e048",
        "outputId": "1a442dd7-e6c2-4072-b827-8e58c6281bd8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2bcf546c-73d2-46b3-98ce-ab86ecae02e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>lead</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50286</th>\n",
              "      <td>sci/tech</td>\n",
              "      <td>Denmark to Claim North Pole, Hopes to Strike O...</td>\n",
              "      <td>Reuters - Denmark aims to claim the North Pole...</td>\n",
              "      <td>Denmark to Claim North Pole, Hopes to Strike O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73587</th>\n",
              "      <td>business</td>\n",
              "      <td>M A Industry Weighs Fees Against Size (Reuters)</td>\n",
              "      <td>Reuters - Investment banks are in the midst of...</td>\n",
              "      <td>M A Industry Weighs Fees Against Size (Reuters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85601</th>\n",
              "      <td>world</td>\n",
              "      <td>Pakistan Wins U.S. Praise Over Afghan Vote</td>\n",
              "      <td>ISLAMABAD (Reuters) - A senior U.S. official ...</td>\n",
              "      <td>Pakistan Wins U.S. Praise Over Afghan Vote  IS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28833</th>\n",
              "      <td>sci/tech</td>\n",
              "      <td>Microsoft exec takes aim at open source</td>\n",
              "      <td>New platform chief, Ashim Pal, says software g...</td>\n",
              "      <td>Microsoft exec takes aim at open source New pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103893</th>\n",
              "      <td>business</td>\n",
              "      <td>Shoppers offer retailers cheer</td>\n",
              "      <td>Retailers generally had a good showing as the ...</td>\n",
              "      <td>Shoppers offer retailers cheer Retailers gener...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bcf546c-73d2-46b3-98ce-ab86ecae02e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bcf546c-73d2-46b3-98ce-ab86ecae02e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bcf546c-73d2-46b3-98ce-ab86ecae02e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           label                                              title  \\\n",
              "50286   sci/tech  Denmark to Claim North Pole, Hopes to Strike O...   \n",
              "73587   business    M A Industry Weighs Fees Against Size (Reuters)   \n",
              "85601      world         Pakistan Wins U.S. Praise Over Afghan Vote   \n",
              "28833   sci/tech            Microsoft exec takes aim at open source   \n",
              "103893  business                     Shoppers offer retailers cheer   \n",
              "\n",
              "                                                     lead  \\\n",
              "50286   Reuters - Denmark aims to claim the North Pole...   \n",
              "73587   Reuters - Investment banks are in the midst of...   \n",
              "85601    ISLAMABAD (Reuters) - A senior U.S. official ...   \n",
              "28833   New platform chief, Ashim Pal, says software g...   \n",
              "103893  Retailers generally had a good showing as the ...   \n",
              "\n",
              "                                                     text  \n",
              "50286   Denmark to Claim North Pole, Hopes to Strike O...  \n",
              "73587   M A Industry Weighs Fees Against Size (Reuters...  \n",
              "85601   Pakistan Wins U.S. Praise Over Afghan Vote  IS...  \n",
              "28833   Microsoft exec takes aim at open source New pl...  \n",
              "103893  Shoppers offer retailers cheer Retailers gener...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Import the AG news dataset (same as hw01)\n",
        "#Download them from here \n",
        "# !wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "path = F\"/content/gdrive/My Drive/Colab Notebooks/Homework/train.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
        "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
        "df = df.sample(n=10000) # # only use 10K datapoints\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a49d6b6e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:20.385383Z",
          "start_time": "2022-03-22T21:40:18.447956Z"
        },
        "id": "a49d6b6e"
      },
      "outputs": [],
      "source": [
        "##TODO tokenize the text, only keep 200 most frequent words \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "##pre-process text \n",
        "def tokenize(x):\n",
        "    return [w.lemma_.lower() for w in en(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
        "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize(x))\n",
        "df[\"preprocessed\"] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
        "                        max_df=.9,  \n",
        "                        max_features=200,\n",
        "                        stop_words='english',\n",
        "                        ngram_range=(1,1))\n",
        "X = vectorizer.fit_transform(df['preprocessed'])\n",
        "vocab = vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c4c0f840",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:23.322875Z",
          "start_time": "2022-03-22T21:40:23.311923Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c0f840",
        "outputId": "f491a4b3-d86d-417b-e100-e53a2f730665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n",
            "[3, 152, 40, 27, 91, 1, 198, 154, 154, 3, 11, 152, 40, 27, 192, 57, 198, 132, 172, 117, 138, 141, 75, 40, 101, 60, 178, 124]\n"
          ]
        }
      ],
      "source": [
        "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the same length (here we use 100)\n",
        "\n",
        "# create one_hot representation for each word\n",
        "!pip install Keras-Preprocessing\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_one_hot = [one_hot(row, n = 200) for row in df[\"preprocessed\"]]\n",
        "print(X_one_hot[0][:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "tyNOhYjVyI0M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyNOhYjVyI0M",
        "outputId": "0b367604-a0e0-4a75-d122-b475b8bb8922"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# next, we pad (or truncate) such that all the inputs have same length\n",
        "max_seq_length = 100\n",
        "X_one_hot_padded = pad_sequences(X_one_hot, padding='post', maxlen=max_seq_length, truncating='post')\n",
        "X_one_hot_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d193dd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:28.364553Z",
          "start_time": "2022-03-22T21:40:28.354695Z"
        },
        "id": "c3d193dd"
      },
      "outputs": [],
      "source": [
        "##TODO create your torch embedding like we did in notebook 5! (hint: predicting labels: world, sport, business, and sci/tech)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "2cac3bd2",
      "metadata": {
        "id": "2cac3bd2"
      },
      "outputs": [],
      "source": [
        "# make dummy variables for the labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "J = encoder.fit_transform(df['label'].astype(str))\n",
        "num_label = max(J)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "VFKROc5A2qe2",
      "metadata": {
        "id": "VFKROc5A2qe2"
      },
      "outputs": [],
      "source": [
        "# set up DNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class EmbeddingNet(nn.Module):\n",
        "  def __init__(self, num_label):\n",
        "    super(EmbeddingNet, self).__init__()\n",
        "    self.embedding = nn.Embedding(num_label, 1) # assign 2 features to each label\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(2, 2)\n",
        "    self.fc2 = nn.Linear(2, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "# I wasn't able to figure out what to do from here onwards and stopped after 1.5h."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H44WDThN3JXv",
      "metadata": {
        "id": "H44WDThN3JXv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
